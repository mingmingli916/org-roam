:PROPERTIES:
:ID:       5E9A97C9-8ED1-435C-A5F2-F35E7CB124C3
:END:
#+title: AdaGrad

AdaGrad is designed to converge rapidly when applied to a convex function.
Comparing to [[id:DC4D2577-D235-4C33-876D-3EB173BA91E2][SGD]], AdaGrad algorithm individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square root of the sum of all of their historical squared values.

\begin{equation}
\boldsymbol{\theta}=\boldsymbol{\theta} - \frac{\epsilon}{\sqrt{\delta \boldsymbol{I}+\operatorname{diag}\left(G \right)}} \odot \boldsymbol{g}
\end{equation}

where $\boldsymbol{\theta}$ is the parameter to be updated, $\epsilon$ is the initial learning rate, $\delta$ is some small quantity that used to avoid the division of zero, $\boldsymbol{I}$ is the identity matrix, $\boldsymbol{g}$ is the gradient estimate.

\begin{equation}
\boldsymbol{G} = \sum_{\tau = 1}^{t} \boldsymbol{g_{\tau} g_{\tau}^{T}}
\end{equation}


AdaGrad shrinks the learning rate according to the entire history of the squared gradient and may have made the learning rate too small before arriving at such a convex structure.
