:PROPERTIES:
:ID:       DC4D2577-D235-4C33-876D-3EB173BA91E2
:END:
#+title: SGD

[[id:F665DF08-2510-4964-8FC2-7EF6043354DC][Differential]]

What is SGD?

SGD stands for stochastic gradient descent.
SGD is a basic optimization algorithm used in machine learning.
It use a minibatch of tanning data to compute the gradient.
Avoiding using the whole training data set can reduce the computation and improve training speed.


Suppose the loss function is
\[
L(\boldsymbol{x,y,\theta})
\]

The estimated gradient is calculated as:
\begin{equation}
\boldsymbol{g} = \frac{1}{m^{'}}\nabla_{\boldsymbol{\theta}} \sum_{i=1}^{m^{'}} L(\boldsymbol{x},y,\boldsymbol{\theta})
\end{equation}


Where $m^{'}$ is the minibatch training data set, $\theta$ is the parameters in the model, $x$ is the input, $y$ is the label.



We adjust the $\theta$ according to the gradient $g$ and learning rate $\epsilon$:
\begin{equation}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \epsilon \boldsymbol{g}
\end{equation}



A crucial parameter for the SGD algorithm is the learning rate.
Previously, we have described SGD as using a fixed learning rate $\epsilon$.
In practice, it is necessary to gradually decrease the learning rate over time, so we now denote the learning rate at iteration $k$ as $\epsilon_k$.


This is because the SGD gradient estimator introduces a source of noise (the random sampling of $m$ training examples) that does not vanish even when we arrive at a minimum.
By comparison, the true gradient of the total cost function becomes small and then 0 when we approach and reach a minimum using batch gradient descent, so batch gradient descent can use a fixed learning rate.
A sufficient condition to guarantee convergence of $\mathrm{SGD}$ is that
\begin{equation}
\sum_{k=1}^{\infty} \epsilon_k=\infty, \quad \text { and }
\end{equation}

\begin{equation}
\sum_{k=1}^{\infty} \epsilon_k^2<\infty
\end{equation}

In practice, it is common to decay the learning rate linearly until iteration $\tau$ :
\begin{equation}
\epsilon_k=(1-\alpha) \epsilon_0+\alpha \epsilon_\tau
\end{equation}

with $\alpha=\frac{k}{\tau}$.
After iteration $\tau$, it is common to leave $\epsilon$ constant.
