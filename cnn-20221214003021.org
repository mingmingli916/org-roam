:PROPERTIES:
:ID:       478601FB-4225-48DB-8482-2DEE1CDEDA6A
:END:
#+title: CNN

[[id:AB5DF84C-7205-48DD-9693-5552E3AB3AAD][LeNet]]
[[id:B41C9F3A-A730-47C0-A292-C11A113C3232][VGG]]
[[id:5B528373-68C5-4F53-943D-9C178ECBA385][SSD]]
[[id:7777ECE9-8A90-42C4-B480-99C001DF0EC6][YOLO]]
[[id:BB705300-BB87-42EA-A774-EC8E235E6BAC][Faster R-CNN]]
[[id:571DBEFF-5BF6-4517-A8DA-2EA6D0C58044][Mask R-CNN]]
[[id:4ACC20C2-B350-4907-BED4-6A5928C288FF][FCN]]
[[id:D7555F4A-B443-4026-8C5C-0C6BAAD8CD4F][Diffusion]]
[[id:89FD966C-7A11-4217-9584-658366CCC5B2][ResNet]]
[[id:4E37508F-F2C3-4D3D-B896-C627C54C14E9][GoogLeNet]]
[[id:8D136245-C668-41B0-9B02-EC0FC96526BA][MLP]]
[[id:5E4AB370-7B8F-4691-98A6-8941AF6FC371][Encoder-Decoder]]
[[id:3ABC4BE1-EF81-4843-8764-EC47560BAD19][GPU]]



* CNN

CNN stands for convolutional neural network.
Convolutional networks are neural networks that have convolutional layers.
A typical convolutional layer consists of three stages:

- Convolution
- [[id:E9CBADED-A24D-43A0-AAB6-6EFFD3AFD805][Activation]]
- [[id:EFE942CF-783B-4AC6-86B1-927F3D680C25][Pooling]]


** Convolution
Convolution is a math operation.
\begin{equation}
  \label{eq:convolution}
  s(t) = \int x(a)w(t-a)da.
\end{equation}

This operation is called *convolution*.
The convolution operation is typically denoted with an asterisk:
\begin{equation}
  s(t) = (x*w)(t).
\end{equation}

In convolutional network terminology, the first argument (in this example, the function $x$) to the convolution is often referred to as the _input_, and the second argument (int this example, the function $w$) as the _kernel_.
The output is sometimes referred to as the _feature map_.

If we assume that $x$ and $w$ are defined only on integer $t$, we can define the discrete convolution:
\begin{equation}
  \label{eq:discrete-convolution}
  s(t) = (x*w)(t) = \sum_{a=-\infty}^{\infty} x(a)w(t-a).
\end{equation}

We often use convolutions over more than one axis at a time.
For example, if we use a two-dimensinal image $I$ as our input, we probably also want to use a two-dimensional kernel $K$:
\begin{equation}
  S(i,j) = (I*K)(i,j) = \sum_m\sum_n I(m,n)K(i-m,j-n).
\end{equation}


The following formula can be used to calculate the output dimension.
\begin{gather}
  h_{o} = \frac{h_{i} - h_{k}}{h_{s}} + 1\\
  w_{o} = \frac{w_{i} - w_{k}}{w_{s}} + 1
\end{gather}
where \(h_{o}\) is the output height, \(h_{i}\) is the input height, \(h_{k}\) is the kernel height, \(h_{s}\) is the stride height, \(w_{o}\) is the output width, \(w_{i}\) is the input width, \(w_{k}\) is the kernel width, \(w_{s}\) is the stride width.

The convolution operation is shown in the following Figure:
#+CAPTION: Convolution operation
[[file:images/conv.png]]





** Properties

CNN leverages three important ideas:

- sparse interaction.
- parameter sharing.
- equivariant representations.


*** Sparse interaction

This is accomplished by making the kernel smaller than the input.


*** Parameter sharing

In convolutional layers, the same parameter defined in one kernel are used at every position of the input.


*** Equivariant representations

In the case of convolution, the particular form of a parameter sharing causes the layer to have a property called _equivariance_ to translation.
To say a function is equivariant means that if the input changes, the output changes in the same way.






